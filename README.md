# deep-learning-challenge
This challenge required me to read data from a csv file from an online source, containing information on organizations that received funding from Alphabet Soup, and prepare the data for creating a relatively accurate model using deep learning neural network techniques. To do this, I loaded in the data, dropped irrelevant columns and chose a good cutoff point, created bins based off of a chosen variable, converted categorical data to numeric using the pd.get_dummies function, split the data into feature and target arrays and then into test and train datasets, and finally scaled the data. After this I created a deep neural net model using the tf.keral.models.Sequential function and created two hidden layers and an outpul layer. The hidden layers had activations of relu while the output had sigmoid. The model was then trained and tested for accuracy. The model was tweaked for a total of three times in the way of changing the activation of the outputs from relu to sigmoid to tanh respectively and the hidden node layer values for each hidden layer was changed. The purpose of this was to try and improve the accuracy of the models to at least 75% which was not achieved.
* Data Preprocessing:
    *Target Variables: the 'IS_SUCCESSFUL' variable is the dependent variable and is the target as we are trying to see how the application        types affect the outcome.
    *Feature Variables:the APPLICATION_TYPE(s) are the featured variables as they are the independent variables we are trying to study the          effectiveness of.
    *Removed Variables: EIN, NAME, STATUS, SPECIAL_CONSIDERATIONS, and ASK_AMT were dropped as they had an excessive amount of number of            unique values that would throw off the accuracy of the model by skewing it. The variables were not needed and so they were dropped.

* Compiling, Training, and Evaluating the Model:
    *Neurons, Layers, and Activation Functions: I used two hidden layers and an outcome layer as I believed adding more hidden layers wouldn't      be beneficial. I used the relu activation functions for the hidden layers and then alternated between relu, sigmoid, and tanh for the          output layer. I adjusted the number of neurons for each model in order to try and find more accurate readings for each of the models          tested.
    *Target Model Performance Achievement: I was unable to achieve the target model performance and had a top accuracy value of about 73%.
    *Increasing Model Performance: As stated above, I mainly adjusted the neuron numbers and hidden/output layer activation function types in      an attempt to increase the performance of the models.

The summary of results is that I was unable to bring the performance of the model up to a minimum accuracy of 75% by the manipulation of the activation function types and number of neurons alone. I also have attempted to increase the epochs but the accuracy levels out around 15-20 times. A recommendation on a different model to try would be to add an additional hidden layer and again test different activation types on the hidden and outcome layers in the goal of achieving increased performance.